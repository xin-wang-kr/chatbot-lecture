{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"\"\"Machine learning is a branch of artificial intelligence and computer science. \n",
    "Machine learning focuses on the use of data and algorithms to imitate the way that humans learn.\"\"\"\n",
    "\n",
    "# Download the punkt tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "\n",
    "# Convert sentences to words\n",
    "sentences = [gensim.utils.simple_preprocess(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the text corpus, we can train the word2vec model using gensim. We can set different parameters for the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the word2vec model\n",
    "w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # input data\n",
    "    vector_size=128,  # size of the vectors\n",
    "    window=5,  # window size\n",
    "    min_count=1,  # minimum count of words\n",
    "    epochs=3,  # number of iterations\n",
    "    hs=0,  # Turn off hierarchical softmax and use negative sampling\n",
    "    sg=1,  # Use skip-gram instead of CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.wv.key_to_index)\n",
    "emb = w2v.wv[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training process, we will get a high-dimensional vector space for text corpus. We can project this vector space into 2D or 3D to further observe the embedding relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensionality\n",
    "reducer = umap.UMAP(n_components=3, random_state=42, n_neighbors=5, metric=\"cosine\")\n",
    "xyz = reducer.fit_transform(emb)\n",
    "\n",
    "# Create a 3D scatter plot with Seaborn\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x=xyz[:, 0]\n",
    "y=xyz[:, 1]\n",
    "z=xyz[:, 2]\n",
    "ax.scatter(x, y, z, c=z, cmap='viridis', marker='o')\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "for i, c in enumerate(vocab):\n",
    "    ax.text(xyz[i,0],xyz[i,1],xyz[i,2],  '%s' % c, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Question and Answer Chatbot Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will create a Q-A chatbot only based on question search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 1: data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D66ZOAoNJouv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/workspaces/word2vec-chatbot-lecture/data/Question_Answer_Dataset_v1.2_S10.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqUJKWBcVH1Q",
    "outputId": "d71f4d05-c6b2-46ea-ec91-1e6f6cbcb491"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DfjBSEcyezOH",
    "outputId": "7f539e50-5cb8-4b9b-c130-f4a017f52de0"
   },
   "outputs": [],
   "source": [
    "df[\"Answer\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "15X6qaDbdBL1",
    "outputId": "dad984b1-2351-4141-95c6-59c1bbe0814f"
   },
   "outputs": [],
   "source": [
    "df[\"Answer\"][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data, there are both text and numbers. We need to generate a set of corpus including both of them. Based on this, we cannot use gensim.utils.simple_preprocess to prepare corpus. Here, we will use regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omm6nuVyfZ2N",
    "outputId": "66e3f0d1-d669-4777-d360-31b5986b7e6a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# initializing string\n",
    "test_str = \"Gfg, is best : for ! Geeks ;? 123 a 9...\"\n",
    "\n",
    "# printing original string\n",
    "print(\"The original string is : \" + test_str)\n",
    "\n",
    "# Removing punctuations in string\n",
    "res = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "# Replace all sequences of two or more spaces with a single space.\n",
    "res = re.sub(' +', ' ', res)\n",
    "\n",
    "# printing result\n",
    "print(\"The string after punctuation filter : \", res.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 2: word2vec model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQrG9X-GVvsT"
   },
   "outputs": [],
   "source": [
    "# train word2vec model with all questions\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = df[\"Question\"].to_list()\n",
    "\n",
    "def token(text):\n",
    "  # Removing punctuations in string\n",
    "  res = re.sub(r'[^\\w\\s]', '', text)\n",
    "  # Replace all sequences of two or more spaces with a single space.\n",
    "  res = re.sub(' +', ' ', res)\n",
    "  # lower case\n",
    "  res = res.lower()\n",
    "  return res.strip().split(\" \")\n",
    "\n",
    "# Convert sentences to words\n",
    "sentences = [token(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rh11_2YWH_Z"
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # input data\n",
    "    vector_size=128,  # size of the vectors\n",
    "    window=5,  # window size\n",
    "    min_count=1,  # minimum count of words\n",
    "    epochs=3,  # number of iterations\n",
    "    hs=0,  # Turn off hierarchical softmax and use negative sampling\n",
    "    sg=1,  # Use skip-gram instead of CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyFpA7ZbYBjP"
   },
   "outputs": [],
   "source": [
    "w2v.save(\"/workspaces/word2vec-chatbot-lecture/data/w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint for assignment_1__: Please rename your trained word2vec model as \"w2v-advanced.model\", when you save it for assignment_1.py. Otherwise, the chatbot application will prompt error or give wrong responses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 3: generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFdwEcdEWNbD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculate sentence vector for each sentence\n",
    "def sentence_vec(sent):\n",
    "    # Filter out terms that are not in the vocabulary from the question sentence\n",
    "    tm_voc = [tm for tm in sent if tm in w2v.wv]\n",
    "    # Get the embedding of the characters\n",
    "    emb = np.vstack([w2v.wv[tm] for tm in tm_voc])\n",
    "    # Calculate the vectors of each included word to get the vector of the question\n",
    "    ave_vec = np.mean(emb, axis=0)\n",
    "    return ave_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Cy2ll2cWOb1"
   },
   "outputs": [],
   "source": [
    "ques_vec = [sentence_vec(sent) for sent in sentences[:df.shape[0]]]\n",
    "ques_vec = np.array(ques_vec)\n",
    "#ans_vec = [sentence_vec(sent) for sent in sentences[df.shape[0]:]]\n",
    "\n",
    "np.savez(\"/workspaces/word2vec-chatbot-lecture/data/vector.npz\", x=ques_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ You can also save answer vectors through giving another array keyword. For example: np.savez(\"/content/sample_data/vector.npz\", x=ques_vec, y=ans_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component 4: vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector search is a machine learning technique that uses numeric representations of content to find similar items in a dataset. Cosine similarity measurement is a common way to conduct vector search.\n",
    "\n",
    "[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's Fundamental AI Research group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYfWon89eThO"
   },
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS similarity search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TEiqSfmeZbr",
    "outputId": "4c29fede-d942-46a5-d384-c9f9694c5fd7"
   },
   "outputs": [],
   "source": [
    "dataSetI = [.1, .2, .3]\n",
    "dataSetII = [.4, .5, .6]\n",
    "#dataSetII = [.1, .2, .3]\n",
    "dataSetIII = [.4, .5, .7]\n",
    "\n",
    "x = np.array([dataSetI]).astype(np.float32)\n",
    "q = np.array([dataSetII]).astype(np.float32)\n",
    "index = faiss.index_factory(3, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "#index.ntotal\n",
    "faiss.normalize_L2(x)\n",
    "index.add(x)\n",
    "y = np.array([dataSetIII]).astype(np.float32)\n",
    "faiss.normalize_L2(y)\n",
    "index.add(y)\n",
    "faiss.normalize_L2(q)\n",
    "distance, index = index.search(q, k=index.ntotal)\n",
    "print('Distance by FAISS:{}'.format(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGi8sK4lezoi",
    "outputId": "ff9d0afe-af02-447b-8a8b-93794607c271"
   },
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6Oc0A8Rgovt",
    "outputId": "edeb2ef6-5dac-473d-a2fb-18f5bb498db9"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "print('Distance by FAISS:{}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate query sentence vector and find matched answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upgqqXODJvoP"
   },
   "outputs": [],
   "source": [
    "def trained_sentence_vec(sent):\n",
    "    # Filter out terms that are not in the vocabulary from the question sentence\n",
    "    qu_voc = [tm for tm in sent if tm in w2v.wv]\n",
    "    # Get the embedding of the characters\n",
    "    emb = np.vstack([w2v.wv[tm] for tm in qu_voc])\n",
    "    # Calculate the vectors of each included word to get the vector of the question\n",
    "    ave_vec = np.mean(emb, axis=0)\n",
    "    return ave_vec\n",
    "\n",
    "def find_answer(qr_sentence, ques_vec):\n",
    "    # use one query sentence to retrieve answer\n",
    "    qr_sentence = gensim.utils.simple_preprocess(qr_sentence)\n",
    "    qr_sent_vec = trained_sentence_vec(qr_sentence)\n",
    "\n",
    "    # perform vector search through similarity comparison\n",
    "    n_dim = ques_vec.shape[1]\n",
    "    x = np.vstack(ques_vec).astype(np.float32)\n",
    "    q = qr_sent_vec.reshape(1, -1)\n",
    "    index = faiss.index_factory(n_dim, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "    faiss.normalize_L2(x)\n",
    "    index.add(x)\n",
    "    faiss.normalize_L2(q)\n",
    "    similarity, idx = index.search(q, k=index.ntotal)\n",
    "    ans_idx = idx[0][0]\n",
    "    return ans_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_sentence = \"What did Alessandro Volta invent in 1800?\"\n",
    "ans_idx = find_answer(qr_sentence, ques_vec)\n",
    "print(\"Query: \", qr_sentence)\n",
    "print(\"Question: \", df[\"Question\"][ans_idx])\n",
    "print(\"Answer: \", df[\"Answer\"][ans_idx])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
