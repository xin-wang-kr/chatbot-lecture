{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "text_data = \"\"\"Machine learning is a branch of artificial intelligence and computer science. \n",
    "Machine learning focuses on the use of data and algorithms to imitate the way that humans learn.\"\"\"\n",
    "\n",
    "# Download the punkt tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "\n",
    "# Convert sentences to words\n",
    "sentences = [gensim.utils.simple_preprocess(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['machine',\n",
       "  'learning',\n",
       "  'is',\n",
       "  'branch',\n",
       "  'of',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'and',\n",
       "  'computer',\n",
       "  'science'],\n",
       " ['machine',\n",
       "  'learning',\n",
       "  'focuses',\n",
       "  'on',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'data',\n",
       "  'and',\n",
       "  'algorithms',\n",
       "  'to',\n",
       "  'imitate',\n",
       "  'the',\n",
       "  'way',\n",
       "  'that',\n",
       "  'humans',\n",
       "  'learn']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the text corpus, we can train the word2vec model using gensim. We can set different parameters for the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the word2vec model\n",
    "w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # input data\n",
    "    vector_size=128,  # size of the vectors\n",
    "    window=5,  # window size\n",
    "    min_count=1,  # minimum count of words\n",
    "    epochs=3,  # number of iterations\n",
    "    hs=0,  # Turn off hierarchical softmax and use negative sampling\n",
    "    sg=1,  # Use skip-gram instead of CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.wv.key_to_index)\n",
    "emb = w2v.wv[vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training process, we will get a high-dimensional vector space for text corpus. We can project this vector space into 2D or 3D to further observe the embedding relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Reduce dimensionality\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensionality\n",
    "reducer = umap.UMAP(n_components=3, random_state=42, n_neighbors=5, metric=\"cosine\")\n",
    "xyz = reducer.fit_transform(emb)\n",
    "\n",
    "# Create a 3D scatter plot with Seaborn\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x=xyz[:, 0]\n",
    "y=xyz[:, 1]\n",
    "z=xyz[:, 2]\n",
    "ax.scatter(x, y, z, c=z, cmap='viridis', marker='o')\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "for i, c in enumerate(vocab):\n",
    "    ax.text(xyz[i,0],xyz[i,1],xyz[i,2],  '%s' % c, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec Question and Answer Chatbot Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will create a Q-A chatbot only based on question search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 1: data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D66ZOAoNJouv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/sample_data/Question_Answer_Dataset_v1.2_S10.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqUJKWBcVH1Q",
    "outputId": "d71f4d05-c6b2-46ea-ec91-1e6f6cbcb491"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleTitle                0\n",
       "Question                    0\n",
       "Answer                      0\n",
       "DifficultyFromQuestioner    0\n",
       "DifficultyFromAnswerer      0\n",
       "ArticleFile                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DfjBSEcyezOH",
    "outputId": "7f539e50-5cb8-4b9b-c130-f4a017f52de0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Napoleon'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Answer\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "15X6qaDbdBL1",
    "outputId": "dad984b1-2351-4141-95c6-59c1bbe0814f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1819'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Answer\"][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data, there are both text and numbers. We need to generate a set of corpus including both of them. Based on this, we cannot use gensim.utils.simple_preprocess to prepare corpus. Here, we will use regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omm6nuVyfZ2N",
    "outputId": "66e3f0d1-d669-4777-d360-31b5986b7e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original string is : Gfg, is best : for ! Geeks ;? 123 a 9...\n",
      "The string after punctuation filter :  ['Gfg', 'is', 'best', 'for', 'Geeks', '123', 'a', '9']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# initializing string\n",
    "test_str = \"Gfg, is best : for ! Geeks ;? 123 a 9...\"\n",
    "\n",
    "# printing original string\n",
    "print(\"The original string is : \" + test_str)\n",
    "\n",
    "# Removing punctuations in string\n",
    "res = re.sub(r'[^\\w\\s]', '', test_str)\n",
    "# Replace all sequences of two or more spaces with a single space.\n",
    "res = re.sub(' +', ' ', res)\n",
    "\n",
    "# printing result\n",
    "print(\"The string after punctuation filter : \", res.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: word2vec model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQrG9X-GVvsT"
   },
   "outputs": [],
   "source": [
    "# train word2vec model with all questions\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = df[\"Question\"].to_list()\n",
    "\n",
    "def token(text):\n",
    "  # Removing punctuations in string\n",
    "  res = re.sub(r'[^\\w\\s]', '', text)\n",
    "  # Replace all sequences of two or more spaces with a single space.\n",
    "  res = re.sub(' +', ' ', res)\n",
    "  # lower case\n",
    "  res = res.lower()\n",
    "  return res.strip().split(\" \")\n",
    "\n",
    "# Convert sentences to words\n",
    "sentences = [token(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rh11_2YWH_Z"
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(\n",
    "    sentences=sentences,  # input data\n",
    "    vector_size=128,  # size of the vectors\n",
    "    window=5,  # window size\n",
    "    min_count=1,  # minimum count of words\n",
    "    epochs=3,  # number of iterations\n",
    "    hs=0,  # Turn off hierarchical softmax and use negative sampling\n",
    "    sg=1,  # Use skip-gram instead of CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyFpA7ZbYBjP"
   },
   "outputs": [],
   "source": [
    "w2v.save(\"/content/sample_data/w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFdwEcdEWNbD"
   },
   "outputs": [],
   "source": [
    "# calculate sentence vector for each sentence\n",
    "def sentence_vec(sent):\n",
    "    # Filter out terms that are not in the vocabulary from the question sentence\n",
    "    tm_voc = [tm for tm in sent if tm in w2v.wv]\n",
    "    # Get the embedding of the characters\n",
    "    emb = np.vstack([w2v.wv[tm] for tm in tm_voc])\n",
    "    # Calculate the vectors of each included word to get the vector of the question\n",
    "    ave_vec = np.mean(emb, axis=0)\n",
    "    return ave_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Cy2ll2cWOb1"
   },
   "outputs": [],
   "source": [
    "ques_vec = [sentence_vec(sent) for sent in sentences[:df.shape[0]]]\n",
    "#ans_vec = [sentence_vec(sent) for sent in sentences[df.shape[0]:]]\n",
    "\n",
    "np.savez(\"/content/sample_data/vector.npz\", x=np.array(ques_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You can also save answer vectors through giving another array keyword. For example: np.savez(\"/content/sample_data/vector.npz\", x=np.array(ques_vec), y=np.array(ans_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector search is a machine learning technique that uses numeric representations of content to find similar items in a dataset. Cosine similarity measurement is a common way to conduct vector search.\n",
    "\n",
    "[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's Fundamental AI Research group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYfWon89eThO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TEiqSfmeZbr",
    "outputId": "4c29fede-d942-46a5-d384-c9f9694c5fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance by FAISS:[[0.9970375 0.9746319]]\n"
     ]
    }
   ],
   "source": [
    "dataSetI = [.1, .2, .3]\n",
    "dataSetII = [.4, .5, .6]\n",
    "#dataSetII = [.1, .2, .3]\n",
    "dataSetIII = [.4, .5, .7]\n",
    "\n",
    "x = np.array([dataSetI]).astype(np.float32)\n",
    "q = np.array([dataSetII]).astype(np.float32)\n",
    "index = faiss.index_factory(3, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "index.ntotal\n",
    "faiss.normalize_L2(x)\n",
    "index.add(x)\n",
    "y = np.array([dataSetIII]).astype(np.float32)\n",
    "faiss.normalize_L2(y)\n",
    "index.add(y)\n",
    "faiss.normalize_L2(q)\n",
    "distance, index = index.search(q, k=2)\n",
    "print('Distance by FAISS:{}'.format(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGi8sK4lezoi",
    "outputId": "ff9d0afe-af02-447b-8a8b-93794607c271"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6Oc0A8Rgovt",
    "outputId": "edeb2ef6-5dac-473d-a2fb-18f5bb498db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance by FAISS:0.9746318461970764\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "result = 1 - spatial.distance.cosine(dataSetI, dataSetII)\n",
    "print('Distance by FAISS:{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUMzZf-8X-Mo",
    "outputId": "0f8e889d-fee1-480e-d146-68322f983b62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 128)\n",
      "(596, 128)\n"
     ]
    }
   ],
   "source": [
    "vector = np.load('/content/sample_data/vector.npz')\n",
    "ques_vec = vector['x']\n",
    "ans_vec = vector['y']\n",
    "print(ques_vec.shape)\n",
    "print(ans_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0-bafj0nEBt"
   },
   "outputs": [],
   "source": [
    "trained_w2v = gensim.models.Word2Vec.load(\"/content/sample_data/w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upgqqXODJvoP"
   },
   "outputs": [],
   "source": [
    "def trained_sentence_vec(sent):\n",
    "    # Filter out terms that are not in the vocabulary from the question sentence\n",
    "    qu_voc = [tm for tm in sent if tm in trained_w2v.wv]\n",
    "    # Get the embedding of the characters\n",
    "    emb = np.vstack([trained_w2v.wv[tm] for tm in qu_voc])\n",
    "    # Calculate the vectors of each included word to get the vector of the question\n",
    "    ave_vec = np.mean(emb, axis=0)\n",
    "    return ave_vec\n",
    "\n",
    "def find_answer(qr_sentence, ques_vec):\n",
    "    # use one query sentence to retrieve answer\n",
    "    qr_sentence = gensim.utils.simple_preprocess(qr_sentence)\n",
    "    qr_sent_vec = trained_sentence_vec(qr_sentence)\n",
    "\n",
    "    # perform vector search through similarity comparison\n",
    "    n_dim = ques_vec.shape[1]\n",
    "    #x = np.vstack(df.ques_vec.values)\n",
    "    x = np.vstack(ques_vec).astype(np.float32)\n",
    "    q = qr_sent_vec.reshape(1, -1)\n",
    "    index = faiss.index_factory(n_dim, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "    faiss.normalize_L2(x)\n",
    "    index.add(x)\n",
    "    faiss.normalize_L2(q)\n",
    "    similarity, idx = index.search(q, k=index.ntotal)\n",
    "    ans_idx = idx[0][0]\n",
    "    return ans_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7PGoM2eKEgi",
    "outputId": "cae1407c-4942-4333-d16d-e976ec9ea5b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Who invented the voltaic pile?\n",
      "Question:  Who is the Mayor of Montreal?\n",
      "Answer:  The mayor is G√©rald Tremblay.\n"
     ]
    }
   ],
   "source": [
    "#qr_sentence = \"Was Alessandro Volta a professor of chemistry?\"\n",
    "#qr_sentence = \"Who did Alessandro Volta marry?\"\n",
    "#qr_sentence = \"Who is Alessandro Volta\"\n",
    "#qr_sentence = \"What did Alessandro Volta invent in 1800?\"\n",
    "qr_sentence = \"Who invented the voltaic pile?\"\n",
    "ans_idx = find_answer(qr_sentence, ques_vec)\n",
    "print(\"Query: \", qr_sentence)\n",
    "print(\"Question: \", df[\"Question\"][ans_idx])\n",
    "print(\"Answer: \", df[\"Answer\"][ans_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMQqBRivmCRx"
   },
   "source": [
    "Use both Question and Answer data for vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYFBV6oimB2X"
   },
   "outputs": [],
   "source": [
    "def trained_sentence_vec(sent):\n",
    "    # Filter out terms that are not in the vocabulary from the question sentence\n",
    "    qu_voc = [tm for tm in sent if tm in trained_w2v.wv]\n",
    "    # Get the embedding of the characters\n",
    "    emb = np.vstack([trained_w2v.wv[tm] for tm in qu_voc])\n",
    "    # Calculate the vectors of each included word to get the vector of the question\n",
    "    ave_vec = np.mean(emb, axis=0)\n",
    "    return ave_vec\n",
    "\n",
    "def find_answer(qr_sentence, ques_vec, ans_vec):\n",
    "    # generate vector for query sentence\n",
    "    qr_sentence = gensim.utils.simple_preprocess(qr_sentence)\n",
    "    qr_sent_vec = trained_sentence_vec(qr_sentence)\n",
    "\n",
    "    # perform vector search through similarity comparison\n",
    "    n_dim = ques_vec.shape[1]\n",
    "    n_q_a = ques_vec.shape[0] # number of pairs of question and answer\n",
    "    x = np.vstack(ques_vec).astype(np.float32)\n",
    "    y = np.vstack(ans_vec).astype(np.float32)\n",
    "    q = qr_sent_vec.reshape(1, -1)\n",
    "    index = faiss.index_factory(n_dim, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "    # add all questions\n",
    "    faiss.normalize_L2(x)\n",
    "    index.add(x)\n",
    "    # add all answers\n",
    "    faiss.normalize_L2(y)\n",
    "    index.add(y)\n",
    "    # do vector search for the query sentence\n",
    "    faiss.normalize_L2(q)\n",
    "    similarity, idx = index.search(q, k=index.ntotal)\n",
    "    ans_idx = idx[0][0]\n",
    "    if ans_idx >= n_q_a:\n",
    "      ans_idx = ans_idx - n_q_a\n",
    "    return ans_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7RMJ6MxpI3D",
    "outputId": "ef910779-0606-424a-c01b-6f71d85f1da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Who invented the voltaic pile?\n",
      "Question:  What did Alessandro Volta invent in 1800?\n",
      "Answer:  In 1800, Alessandro Volta invented the voltaic pile.\n"
     ]
    }
   ],
   "source": [
    "#qr_sentence = \"Was Alessandro Volta a professor of chemistry?\"\n",
    "#qr_sentence = \"Who did Alessandro Volta marry?\"\n",
    "#qr_sentence = \"Who is Alessandro Volta\"\n",
    "#qr_sentence = \"What did Alessandro Volta invent in 1800?\"\n",
    "qr_sentence = \"Who invented the voltaic pile?\"\n",
    "ans_idx = find_answer(qr_sentence, ques_vec, ans_vec)\n",
    "print(\"Query: \", qr_sentence)\n",
    "print(\"Question: \", df[\"Question\"][ans_idx])\n",
    "print(\"Answer: \", df[\"Answer\"][ans_idx])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
